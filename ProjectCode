import bs4 as bs
import requests
import numpy
import pandas
import csv

#2004
url = 'https://en.wikipedia.org/wiki/Deaths_in_January_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jan04 = []
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jan04 = Jan04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_February_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Feb04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Feb04 = Feb04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_March_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Mar04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Mar04 = Mar04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_April_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Apr04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Apr04 = Apr04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_May_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
May04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    May04 = May04 + observations
    i = i + 1
#


url = 'https://en.wikipedia.org/wiki/Deaths_in_June_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jun04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jun04 = Jun04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_July_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jul04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jul04 = Jul04 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_August_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Aug04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Aug04 = Aug04 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_September_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Sep04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Sep04 = Sep04 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_October_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Oct04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Oct04 = Oct04 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_November_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Nov04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Nov04 = Nov04 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_December_2004'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Dec04 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Dec04 = Dec04 + observations
    i = i + 1
#
#2005
url = 'https://en.wikipedia.org/wiki/Deaths_in_January_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jan05 = []
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jan05 = Jan05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_February_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Feb05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Feb05 = Feb05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_March_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Mar05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Mar05 = Mar05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_April_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Apr05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Apr05 = Apr05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_May_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
May05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    May05 = May05 + observations
    i = i + 1
#


url = 'https://en.wikipedia.org/wiki/Deaths_in_June_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jun05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jun05 = Jun05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_July_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jul05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jul05 = Jul05 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_August_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Aug05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Aug05 = Aug05 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_September_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Sep05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Sep05 = Sep05 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_October_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Oct05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Oct05 = Oct05 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_November_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Nov05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Nov05 = Nov05 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_December_2005'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Dec05 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Dec05 = Dec05 + observations
    i = i + 1
#
#2006
url = 'https://en.wikipedia.org/wiki/Deaths_in_January_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jan06 = []
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jan06 = Jan06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_February_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Feb06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Feb06 = Feb06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_March_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Mar06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Mar06 = Mar06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_April_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Apr06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Apr06 = Apr06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_May_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
May06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    May06 = May06 + observations
    i = i + 1
#


url = 'https://en.wikipedia.org/wiki/Deaths_in_June_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jun06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jun06 = Jun06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_July_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Jul06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Jul06 = Jul06 + observations
    i = i + 1
#

url = 'https://en.wikipedia.org/wiki/Deaths_in_August_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Aug06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Aug06 = Aug06 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_September_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Sep06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Sep06 = Sep06 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_October_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Oct06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Oct06 = Oct06 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_November_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Nov06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Nov06 = Nov06 + observations
    i = i + 1
#
    
url = 'https://en.wikipedia.org/wiki/Deaths_in_December_2006'
re = requests.get(url)
soup = bs.BeautifulSoup(re.content, 'lxml')
death = soup.find_all('ul')

i = 2
Dec06 = []    
while i < len(death)-36:
    observations = death[i].text.split('\n')
    Dec06 = Dec06 + observations
    i = i + 1
#
#2007







    
NotableDeath = numpy.concatenate((Jan04,Feb04,Mar04,Apr04,May04,Jun04,Jul04,Aug04,Sep04,Oct04,Nov04,Dec04,Jan05,Feb05,Mar05,Apr05,May05,Jun05,Jul05,Aug05,Sep05,Oct05,Nov05,Dec05,Jan06,Feb06,Mar06,Apr06,May06,Jun06,Jul06,Aug06,Sep06,Oct06,Nov06,Dec06), axis=0)

#name
i = 0
allname = []
while i < len(NotableDeath):
    first_comma = NotableDeath[i].find(',')
    name = NotableDeath[i][0:first_comma]
    allname.append(name)
    i = i + 1
print(allname)

#underscore name
i = 0
underscore = []
while i < len(allname):
    name_ = allname[i].replace(" ", "_")
    underscore.append(name_)
    i = i + 1
print(underscore)


#age
i = 0
allage = []

while i < len(NotableDeath):
        
    age = NotableDeath[i].split(",")
    try:
        allage.append(age[1])
    except IndexError:
        allage.append(age[0])
#    except len(allage[i])>10:
#        allage.append(age[2])

    i = i + 1
print(allage)
#print(len(allage[4]))
#Age to interger    
newage = []
for s in allage:
    try:    
        age = int(s)
    except ValueError:
        age = -1 

    newage.append(age)
    
print(newage)

#Nationality    
i = 0
nation = []
while i < len(NotableDeath):
        
    nat = NotableDeath[i].split(",")
    try:
        nation.append(nat[2])
#    except IndexError:
#        nation.append(nat[1])
    except IndexError:
        nation.append(nat[0])


    i = i + 1
print(nation)

demonym = [nat.split()[0] for nat in nation]
print(demonym)

with open('NotableDeathProject.csv', 'w', newline='', encoding="utf-8") as csvfile:
    fieldnames = ['PAGE_TITTLE', 'NAME', 'AGE', 'DEMONYM_1', 'DESCRIPTION']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()


    for i in range(len(allname)):
        writer.writerow({'PAGE_TITTLE':underscore[i], 'NAME': allname[i], 'AGE': newage[i], 'DEMONYM_1': demonym[i], 'DESCRIPTION': NotableDeath[i]})
#    Take out the end and begining
    
    
